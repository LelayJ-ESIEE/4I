{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD - Vectorisation de texte, représentation graphique et classification\n",
    "\n",
    "Dans ce TD, nous nous intéressons à la représentation vectorielle de texte à partir de la méthode [Bag of words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Nous étudierons les effets des différents paramètres sur la représentation obtenue. Nous étudierons également deux méthodes pour représenter graphiquement des données vectorielles de grande dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys,os,os.path\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'ensemble de textes : SMS Spam Collection\n",
    "\n",
    "Dans ce TD, nous utiliserons l'ensemble de textes qui se nomme « SMS Spam Collection v1. ». C'est une collection d'environ 5600 SMS, répartis en 2 groupes (spam et ham).\n",
    "\n",
    "Pour plus de detaille : [site officiel](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Pour chargez l'ensemble de textes, utiliez le code suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dataset = pd.read_csv('SMSSpamCollection.txt', sep=\"\\t\", header=None).rename(index=str, columns={0: \"label\", 1: \"text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Vous pouvez afficher un échantillons du dataset de SMS et son analyse avec les codes suivants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** L'ensemble de textes est-il équilibré ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "### Création du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Créez un `CountVectorizer` et utilisez le corpus de texte pour apprendre le dictionnaire de vectorisation ([CountVectorizer.fit()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Quelle est la taille du dictionnaire obtenu (`CountVectorizer.vocabulary_`) ? Affichez les mots du dictionnaire, que pouvez-vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de mots dans le dictionnaire\n",
    "\n",
    "Dans un premier temps, nous utiliserons les méthodes vue en cours pour réduire le nombre de mots dans le dictionnaire.\n",
    "\n",
    "**À faire -** Àjoutez l'arguement `stop_words='english'` à la contruction du `CountVectorizer` pour retirer les stop words du dictionnaire.\n",
    "\n",
    "**Question -** Quelle est la taille du dictionnaire alors obtenu ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nous avons vu en cours qu'il existe pour chaque langue et pour un grand nombre de mots de cette langue un sous-ensemble de mots dérivés.  Pour réduire la taille du vocabulaire, nous conservons l'élément racine de ces sous ensembles.\n",
    "\n",
    "Nous allons utiliser la méthode `SnowballStemmer` pour extraire de la racine des mots (stemming).\n",
    "\n",
    "**À exécuter -** Nous surchargons les classes `CountVectorizer` et `TfidfVectorizer` pour àjouter l'extraction de la racine des mots dans leures méthodes d'analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "class EnglishStemmedCountVectorizer(CountVectorizer): #EnglishStemmedCountVectorizer hérite de CountVectorizer\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(EnglishStemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "class EnglishStemmedTfidfVectorizer(TfidfVectorizer):#EnglishStemmedTfidVectorizer hérite de TfidVectorizer\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(FrenchStemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -** Utiliez le vectorizer `EnglishStemmedCountVectorizer`, que nous venons de créer et àjoutez l'arguement `stop_words='english'` à sa contruction pour retirer les stop words du dictionnaire.\n",
    "\n",
    "**Question -** Quelle est la taille du dictionnaire alors obtenu ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, pour réduire le nombre de mots dans le dictionnaire, nous allons étudier la **fréquence d'apparition des mots dans les documents** ou *document frequency* en anglais.\n",
    "\n",
    "**Question -** Utilisez la fonction [CountVectorizer.transform()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform) pour vectoriser les documents du corpus de texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Pour calculer la fréquence d'apparition de chaque mot dans les documents à partir des documents vectoriser, nous utiliserons la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_frequency(X):\n",
    "    df = np.mean(X>0, axis=0)\n",
    "    df = np.asarray(df).reshape(-1) # convertie un numpy.matrix en numpy.ndarray\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Pour affichez l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte, nous utiliserons la fonction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_df(df, bins=None, yscale='linear'):\n",
    "    fig = plt.figure(figsize=(6,6));\n",
    "    ax = fig.gca();\n",
    "    ax.hist(df,bins);\n",
    "    ax.set_yscale(yscale);\n",
    "    ax.set_xbound([np.min(df),np.max(df)]);\n",
    "    ax.set_xlabel('Document frequency');\n",
    "    ax.set_ylabel('Nombre de mot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** À l'aide des deux fonction que nous venons de définir, affichez l'histogramme de la fréquence d'apparition des mots dans les documents de notre ensemble de SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Qu'est-ce que cela signifie ? Est-il intéressant d'avoir des mots avec une fréquence proche de 1 ou de 0 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** A l'aide de la fonction [CountVectorizer.inverse_transform()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.inverse_transform), quels sont les mots du dictionnaire qui ont une fréquence trop petite (par exemple avec un df < 0.001) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** A l'aide des parametres **`min_df`** et **`max_df`** de la classe [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), filtrer les mots avec des fréquences indésirables.\n",
    "\n",
    "Quelle est la taille du dictionnaire alors obtenu ?\n",
    "\n",
    "Affichez alors le nouvel l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte ([matplotlib.pyplot.hist()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Nous avons vu en cours qu'il est intéressant de visualiser les données pour juger de la qualité de la représentation. \n",
    "\n",
    "**À exécuter -**  Pour visualiser en regardant les dimensions deux à deux, nous utilions la fonction suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_croises(X,y,target_names, threshold = 1.0):\n",
    "    uy = np.unique(y)\n",
    "    nb_class=len(uy)\n",
    "    nb_feat=X.shape[1]\n",
    "    if nb_feat%2:\n",
    "        ncols=int((nb_feat-1)/2)\n",
    "        nrows=nb_feat\n",
    "    else:\n",
    "        ncols=int(nb_feat/2)\n",
    "        nrows=nb_feat-1\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    col=['b','r','g','c','m','y','k']\n",
    "    for t in range(nb_class):\n",
    "        k=1\n",
    "        for i in range(nb_feat-1):\n",
    "            for j in range(i+1,nb_feat):\n",
    "                plt.subplot(nrows,ncols,k)\n",
    "                \n",
    "                x_t = X[y == uy[t],i]\n",
    "                y_t = X[y == uy[t],j]\n",
    "                \n",
    "                x_mean = np.mean(x_t)\n",
    "                y_mean = np.mean(y_t)\n",
    "                \n",
    "                dist = (x_t-x_mean)**2 + (y_t-y_mean)**2\n",
    "                \n",
    "                \n",
    "                dist_max = np.sort(dist)[int(np.floor((dist.shape[0]-1)*threshold))]\n",
    "                \n",
    "                plt.scatter(x_t[dist<dist_max], y_t[dist<dist_max], color=col[t],lw=2,marker='o',label=target_names[t], alpha=0.5)\n",
    "                plt.ylabel('Dim. ' + str(j+1))\n",
    "                plt.xlabel('Dim. ' + str(i+1))\n",
    "                k=k+1\n",
    "    plt.legend(bbox_to_anchor=(2,2))  \n",
    "    plt.tight_layout()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Nous pouvons alors visualiser les 6 premières dimensions de notre représentation par Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_croises(X[:,:6],sms_dataset['label'],sms_dataset['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Que constatez vous ?\n",
    "\n",
    "### Analyse en Composantes Principales\n",
    "\n",
    "Une méthodes tres couramments utilisée qui permet de visualiser les données est l'utilisation de [Analyse en Composantes Principales](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)(ACP), qui est disponible dans [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** A l'aide de l'ACP, réduisez la dimension de la signature à 6 dimensions et visualisez-les deux à deux avec la fonction `plot_croises()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Le résultat obtenu est-il plus facilement lisible ? Pourquoi ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination par régression logistique\n",
    "\n",
    "### Mise en oeuvre\n",
    "\n",
    "Nous aurons besoin de la classe `LogisticRegression`, d'une méthode découpant en un ensemble d'apprentissage et de test, `train_test_split` et éventuellement des méthodes d'analyse et rapport `confusion_matrix`, `classification_report`.\n",
    "\n",
    "**À exécuter -** Celles-ci sont importées par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -**\n",
    " - définir une base de test et une base d'apprentissage en utilisant la méthode `train_test_split`\n",
    " - instancier la classe `LogisticRegression`, par exemple sous le nom `cls`, puis apprendre par la méthode `fit` et prédire par `predict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -** Visualiez quels sont les coefficients obtenus par `cls.intercept_` et `cls.coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -** Utiliez la fonction `classification_report` pour visualiez les performances de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À exécuter -** Pour afficher la matrice de confusion, nous utiliserons la fonction suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -** Utiliez la fonction `confusion_matrix` pour calculer la matrice de confusion et utiliez la fonction que nous venons de définir pour l'afficher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Analysez la matrice de confusion obtenue, le résultat est-il cohérent avec la visualisation obtenue en utilisant l'ACP ?\n",
    "\n",
    "**À faire -** Àjoutez l'arguement `class_weight='balanced'` à la contruction de `LogisticRegression`.\n",
    "\n",
    "**Question -** Observez les effets sur les performances de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À faire -** Faites varier le parametre **`min_df`** de la classe [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "**Question -** Observez les effets sur la visualisation et sur les performances de classification"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
